---
title: "Assignment V: GitHub and the ticketmaster.com API"
subtitle: "Data Science Project Management | Winter Term 2020/21"
author: "Submitted by Franziska MÃ¼ller (Student ID: 5401673)"
date: "February, 16, 2021"
output: 
  html_document:
  toc: true
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
I hereby assure that my submission is in line with the *Code of Conduct* outlined on the lecture slides. I worked on my own.

## General setup

Before I start the project, I clear my workspace. 
# Note regarding key and maybe working directory

```{r preparation, message = FALSE, warning = FALSE}
# Clear workspace
rm(list = ls())

# Source private key - insert yout indicidual path to the key
source("C:/Users/Francesca/Desktop/MA_WiSe 202021/Data Science Project Management/Assignments/key_ticketmaster.R")
```

## Exercise 1: Setting up a GitHub repository
Please find my repository via: https://github.com/FrancescaMueller/DSPM_AssignmentV.git

## Exercise 2: Getting to know the API
Rate limit is given by 5 requests per second and max. 5,000 requests per day.

## Exercise 3: Interacting with the API - the basics

*Load the packages needed to interact with APIs using R.*
```{r packages, message = FALSE, warning = FALSE}
# Check if packages have been installed before; if not, install them
if (!require("jsonlite")) install.packages("jsonlite")
if (!require("httr")) install.packages("httr")
if (!require("rlist")) install.packages("rlist")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("tidyr")) install.packages("tidyr")
if (!require("dplyr")) install.packages("dplyr")

# Load packages
library(jsonlite) 
library(dplyr)
library(httr)
library(rlist)
library(tidyverse)
library(tidyr)
```

*Perform a first GET request, that searches for event venues in Germany (countryCode = "DE"). Extract the content from the response object and inspect the resulting list. Describe what you can see.*

```{r first_GET_request}

my_url <- paste0("https://app.ticketmaster.com/discovery/v2/venues?apikey=", key)

first_glimpse <- function(country, page) {
  resp_obj <- GET(url = paste0(my_url, 
                               "&locale=*&page=", 
                               page, 
                               "&countryCode=", 
                               country))
  cont_obj <- resp_obj %>% content()
  return(cont_obj)
}

country <- 'DE'
Ex3_first_glimpse <- first_glimpse(country, 0)
# Use page = 0 in order to get the first 20 results

```
The resulting content object is a list with nested structure. It contains the lists named ``r names(Ex3_cont_obj)[1]``, ``r names(Ex3_cont_obj)[2]`` and `` names(Ex3_cont_obj)[3]``. The first one covers different venues and detailed information about a specific venue such as name, type, postal code etc. Interesting here, the lists have different lenghts. Therefore, not every information is given for every venue.
The second one contains links regarding the first, self (actual scraped page), next and last page. Here, it is shown that there are 3465 elements in total with 20 results each. In the last one also information about pages and results is given but just the raw information without the respective url. Again, there are ``r Ex3_cont_obj[["page"]][["totalPages"]]`` pages with ``r Ex3_cont_obj[["page"]][["size"]]`` elements each (in total: ``r Ex3_cont_obj[["page"]][["totalElements"]]`` elements).


*Extract 'name', 'city', 'postalCode', 'adress', 'url', longitude', 'latitude'.*
```{r get_content_function, warning = FALSE}
my_url <- paste0("https://app.ticketmaster.com/discovery/v2/venues?apikey=", key)
country <- "DE"

# Function GET CONTENT
get_content <- function(country, page) {
  # Note_ n encoding supplied: defaulting to UTF8
  resp_obj <- GET(url = paste0(my_url, 
                               "&locale=*&page=", 
                               page, 
                               "&countryCode=", 
                               country))
  cont_obj <- fromJSON(content(x = resp_obj, 
                               as = 'text'))
  
  # Keep just relevant information about venues in general 
  placeholder <- as.data.frame(cont_obj$`_embedded`$venues)  # HERE -> some observations have no locations - BUT error if a whole page does not have variable location!
  
  # Just keep relevant information asked by the assignment
  df <- data.frame(placeholder$name, placeholder$city[1],
                   placeholder$postalCode, placeholder$address, placeholder$url
                  # placeholder$location[1], placeholder$location[2]
                   )
  
  # Adjust for location
  if (is.null(placeholder$location) == TRUE) {
    df['longitude'] <- NA
    df['latitude'] <- NA
  } else{
    df <- df %>% cbind(placeholder$location[1], placeholder$location[2])
  }
  
  # Rename columns
  names(df) <- c("name", "city", "postalCode", "address", "url", "longitude", "latitude")
  
  # Change longitude and latitude to type double
  df$longitude <- df$longitude %>% as.double()
  df$latitude <- df$latitude %>% as.double()
  
  return(df)
}

Ex3 <- get_content(country, 0)

```

## Exercise 4: Interacting with the API - advanced

*Have a closer look at the element list named ``page``.*
There are ``r Ex3_cont_obj[["page"]][["totalPages"]]`` pages with ``r Ex3_cont_obj[["page"]][["totalElements"]]`` elements in total. 
Note: The loop must be enriched with further statements to get the elements from the last page
Use the parameter ``page`` to access the different pages. 

```{r Ex_4}
m <- as.numeric(Ex3_first_glimpse[["page"]][["totalElements"]])  # Number of total elements
pages <- as.numeric(Ex3_first_glimpse[["page"]][["totalPages"]])  # Number of total pages
# n <- as.numeric(nrow(Ex3))  # Number of elements per page

# Create empty data frame
df_Ex4 <- tibble(name = character(m),
                 city = character(m),
                 postalCode = character(m),
                 adress = character(m),
                 url = character(m),
                 longitude = double(m),
                 latitude = double(m))

# Loop 
all_pages <- function(country, number_pages, df_all_pages) {
  for (j in 0:number_pages) {
    tryCatch({  
      # added since very error prone
      cont_obj <- get_content(country, j)
      df_all_pages[(20 + 20 * j - 19):(20 + 20 * j),] <- cont_obj
      Sys.sleep(time = 0.3)  # 5 requests per second, but adjusted
    },
    error = function(e){})
    }
  return(df_all_pages)
  }

# Get info all pages
system.time(
  Ex4_all <- all_pages(country, pages, df_Ex4)
) # 127.25

# Save created data frame
saveRDS(Ex4_all, file = "./data_DE.rds")

```
